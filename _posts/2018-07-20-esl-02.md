---
layout: post
title: Trevor Hastie，統計的学習の基礎：2章，教師あり学習の基礎
updated: 2018-07-20
cover:  "/assets/2018-07-20-lake.jpg"
categories:
 - machine learning
 - statistics
 - books
---

夏休みの宿題として，[Trevor Hastie，統計的学習の基礎 ―データマイニング・推論・予測―](http://amzn.asia/ay0yxvo)を勉強している．以下は，**第2章：教師あり学習の概要** のメモ．

# 変数の種類と用語

- 順序付きカテゴリカル変数：値の順序のみに意味があり，計量としての意味がない変数．
- 記法：
  + $$X$$：入力変数．
  + $$Y$$：量的な出力．
  + $$G$$：質的な出力．
  + 変数そのものを指す場合は大文字（$$X$$），観測値を指す場合は小文字（$$x$$）．
  + 行列を指す場合は太字の大文字（$$\mathbf{X}$$），ベクトルを指す場合は，単なる大文字（$$x$$）．
  + ベクトル$$X$$の$$i$$番目の要素の$$n$$個の観測値をまとめてベクトルで表す場合は，太字で表す（$$\mathbf{x}_i$$）．
  + 予測値を指す場合は，ハットを用いる（$$\hat{Y}$$）．

# 予測のための二つの簡単なアプローチ：最近傍法

- データ数が$$N$$の$$k$$最近傍法において，有効パラメータ数は$$N/k$$．
- 最近傍法の拡張：
  + 最近傍法では各点の重みを$$\{0, 1\}$$で表すが，カーネル平滑化では滑らかな連続変数で表す．
  + 高次元データの場合，距離尺度を特徴づけるカーネルに修正を加える．
  + 最近傍法では近傍領域を定数モデルで表すが，局所重み付け最小二乗法を用いた局所線形モデルでは線形モデルで表す．
  + 入力変数を基底展開して線形モデルを構築すると任意の複雑なモデルを構築できる．
  + 射影追跡法やニューラルネットワークは，非線形変換と線形モデルを構築したもの．

# 統計的決定理論

- 期待予測誤差（Expected Prediction Error）：$$\mathrm{EPE}(f) = \int \left(y - f(x) \right)^2 \mathrm{Pr} (dx, dy)$$
- EPEを最小化するのは，条件付き期待値：$$\mathrm{E}(Y \mid X=x)$$
- 条件付け：同時分布を$$\mathrm{Pr}(X, Y) = \mathrm{Pr}(Y \mid X)\mathrm{Pr}(X)$$のように分解すること．
- 十分な大きさの訓練データが得られれば，$$\hat{f}(x) \rightarrow \mathrm{E}(Y \mid X=x)$$となり，万能な近似モデルとなる．
- $$\mathrm{card}(\mathcal{G})$$は，集合$$\mathcal{G}$$の要素数を表す．

# 高次元での局所的手法

- バイアス-分散分解．平均二乗誤差（MSE）を分解すると，次のようになる．$$\mathrm{MSE}(x_0) = \mathrm{E}_{\tau}[f(x_0)-\hat{y}_0]^2 = \mathrm{Var}_{\tau}(\hat{y}_0) + \mathrm{Bias}^2(\hat{y}_0)$$．ここで，$$\tau$$は訓練データ集合を表す．
- 制限の強い仮定に基づく線形モデルと，柔軟な最近傍法は両極端な性質を持つ．以降，これらの中間的な性質を持つ方法を学ぶ．

# 統計モデル，教師あり学習，関数近似

- 線形基底展開：$$f_{\theta}(x) = \sum_{k=1}^{K}h_k (x) \theta_k$$．
  + $$h_k$$は入力ベクトル$$x$$に関する関数や変換を表す．
  + ニューラルネットワークはこの一種であり，$$h_k(x) = \frac{1}{1 + \mathrm{exp}(-x^{T}\beta_k)}$$．
- 残差二乗和（RSS; Sum of Squared Residuals）：$$\mathrm{RSS}(\theta) = \sum_{i=1}^N (y_i - f_{\theta}(x_i))^2$$
- 最尤推定：対数尤度関数を最大化するパラメータ$$\theta$$を採用する．
  + 量的出力：$$L(\theta) = \sum_{i=1}^N \mathrm{log} \mathrm{Pr}_{\theta} (y_i)$$を最小化する．ここで，$$\mathrm{Pr}_{\theta}(y_i)$$は，パラメータ$$\theta$$で特徴づけられた$$Y$$の確率密度関数．加法的誤差モデル（$$Y = f_{\theta} (X) + \epsilon$$）において，$$\epsilon \sim \mathcal{N}(0, \sigma^2)$$ならば，最尤推定は最小二乗法と等価になる．
  + 質的出力：$$L(\theta) = \sum_{i=1}^N \mathrm{log} p_{g_i, \theta} (x_i)$$を最小化する．ここで，$$p_{k, \theta} (x) = \mathrm{Pr}(G=\mathcal{G}_k \mid X=x)$$．交差エントロピーとも呼ぶ．

# 構造化回帰モデル

- 残差二乗和$$\mathrm{RSS}(f) = \sum_{i=1}^N (y_i - f(x_i))^2$$に基づいて関数$$f$$を推定するとする．
- データ数$$N$$が有限の場合，関数$$f$$を一意に求めるために，学習に対して制約を設ける．
  + 制約の強さ：近傍領域の大きさ．
  + 制約の複雑さ：近傍領域で想定する規則性．
- 等方性の小さい領域を近傍とする局所的な方法は，どのようなものであっても高次元データにうまく適用できない．
- ノンパラメトリックな推定のためには，大きく分けて３つの方法がある．
  + 粗度に対する罰則：残差二乗和にペナルティ項を追加する．$$\mathrm{PRSS}(f; \lambda) = \mathrm{RSS}(f) + \lambda J(f)$$．射影追跡回帰や正則化など．ベイジアンの枠組みで理解することができる．
  + カーネル関数による局所関数近似：近傍領域をカーネル関数で表現し，明示的に回帰関数や条件付き期待値を推定する．
  + Dictionary method：基底関数を用いて関数を表現する．$$f_{\theta}(x) = \sum_{m=1}^M \theta_m h_m (x)$$．

# モデル選択と，バイアスと分散のトレードオフ

- モデルの複雑度が増すと分散が増え，複雑度が減るとバイアスが増える．
